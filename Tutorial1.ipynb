{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 1: An introduction to event-based processing\n",
    "Prepared by the International Centre for Neuromorphic Systems (ICNS) at Western Sydney University for the AMOS Conference 2022.\n",
    "\n",
    "## Introduction\n",
    "***What you will learn***\n",
    "In this tutorial, we will work with real-world event-based data to learn how to load and process it. \n",
    "\n",
    "By the end of this tutorial, you will...\n",
    "* Be able to load and read events from an event-based data file.\n",
    "* Understand the data representation from an event-based camera and how to interact with it.\n",
    "* Extract important information for the event-based data stream.\n",
    "* Be able to write an iterative event-based algorithm for processing event-based data.\n",
    "\n",
    "We'll be using the following recording for the exercises in this section:\n",
    "\n",
    "![SegmentLocal](images/two_horses_denoised_exponential_frametime_20ms_tau_100ms.gif \"segment\")\n",
    "\n",
    "This recording was chosen as it shows:\n",
    "\n",
    "- clearly visible and identifiable objects (i.e. the horse and the person)\n",
    "\n",
    "- the importance of motion in a visual scene - without it, the images in each static frame are not interpretable (i.e. they don't look like a horse or a person)\n",
    "\n",
    "This tutorial aims to demonstrate this further through the following exercises.  To do these exercises, a few set-up steps are required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "\n",
    "Firstly, we will need to install ```event_stream```, a library which provides an easy method for interacting with [Event Stream](https://github.com/neuromorphic-paris/event_stream) files (formatted .es) containing data from an event-based camera.\n",
    "\n",
    "NOTE: Event Stream is not the only file format for event-based data.  There are tools to assist in converting from other formats to ```Event Stream```, including the open-source [loris](https://github.com/neuromorphic-paris/loris) tool.  We will exclusively use the .es file format here.\n",
    "\n",
    "Libraries and sample code are available for Matlab, Python, and C++ at the [official repository](https://github.com/neuromorphicsystems/event_stream). \n",
    "\n",
    "\n",
    "For this tutorial, we will be using the Python interface.  Event Stream needs numpy to run and we will use matplotlib to display results.  If you do not have these installed, you will need to do that now though:\n",
    "\n",
    "```bash\n",
    "    pip install numpy\n",
    "```\n",
    "\n",
    "```bash\n",
    "    pip install matplotlib\n",
    "```\n",
    "\n",
    "Run the following command to install this library:\n",
    "\n",
    "```bash\n",
    "    pip install event_stream\n",
    "```\n",
    "\n",
    "**Note:** If you are using Python in Anaconda, matplotlib may already be installed, but it's worth checking.  Don't forget to use conda instead of pip!!\n",
    "\n",
    "You may get errors during the installation of the ```event_stream``` library if the installed version of ```numpy``` is not compatible. If this occurs, the easiest solution is to use a tool like ```virtualenv``` or ```conda``` to create a new virtual environment for Python.  Instructions on how to do this can be found in the README file.\n",
    "\n",
    "\n",
    "Once these two tools are installed, we can begin to explore event-based data using some example data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing event_stream\n",
    "\n",
    "We will start by loading some event-based data using the ```Event Stream``` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import event_stream\n",
    "\n",
    "decoder = event_stream.Decoder(\"Data/WSU/two_horses.es\")\n",
    "\n",
    "print(f\"Decoder type: {decoder.type}\")\n",
    "print(f\"Sensor resolution: {decoder.width} x {decoder.height}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above loads the data from an event-based file and prints out some important information about the data contained within the file. \n",
    "\n",
    "**If this example runs, you have successfully configured your Python environment.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Decoders\n",
    "\n",
    "The ```event_stream``` library uses the concepts of encoders and decoders to work with event-based files. \n",
    "\n",
    "**Decoders** interpret files and produce a stream of events for processing, whilst encoders write new event-based data to a data file. \n",
    "\n",
    "Event-based data is asynchronously generated, meaning that there is not fixed frame-rate or event-rate. The activity in the visual scene determines the number of events generated during any period. This complicates the process of saving and recalling data as the data does not appear in convenient and organised structures such as frames. In fact, we tend to store event-based in a way that allows it to be read out iteratively, event-by-event. In principle, this is how the standard decoder in ```event-stream``` operates. \n",
    "\n",
    "The ```decoder.type``` property shows the type of the event-based file. Whilst the underlying concepts around event-based cameras are broadly similar for all the sensors available, there are sensors that implement additional functionality which also produces data in an event-based fashion.\n",
    "\n",
    "The most common decoder type (and the one used throughout these tutorials) is the ```dvs``` data type.\n",
    "\n",
    "In practice, reading the data event-by-event from the file is inefficient as it incurs a significant overhead on each event. For performance reasons, we tend to buffer the data into groups of events. The ```event_stream``` decoder implements this by reading in chunks of events and then allowing you to iterate through them. \n",
    "\n",
    "We can explore this behaviour by interacting with the decoder as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import event_stream\n",
    "\n",
    "decoder = event_stream.Decoder(\"Data/WSU/two_horses.es\")\n",
    "\n",
    "# Read the first chunk of events from the file. \n",
    "# The decoder is an iterator, and we can retrieve the first chunk of events by using the next function.\n",
    "chunk = next(decoder)\n",
    "print(f\"The first chunk contains {len(chunk)} events.\")\n",
    "\n",
    "# Manually print out the first three events in this chunk\n",
    "print(f\"x: {chunk[0]['x']}, y: {chunk[0]['y']}, t: {chunk[0]['t']}, p: {chunk[0]['on']}\")\n",
    "print(f\"x: {chunk[1]['x']}, y: {chunk[1]['y']}, t: {chunk[1]['t']}, p: {chunk[1]['on']}\")\n",
    "print(f\"x: {chunk[2]['x']}, y: {chunk[2]['y']}, t: {chunk[2]['t']}, p: {chunk[1]['on']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Address-Event Representation (AER)\n",
    "\n",
    "Event-based cameras output data in a format known as Address-Event Representation (AER). This format is widely used in neuromorphic systems to pass data from one system to another. The unit of data exchange in an AER system is an event, which indicates a message from a specific source, which is a specific pixel in the case of an event-based sensor.\n",
    "\n",
    "Each AER event from the camera is represented by four values:\n",
    "\n",
    "$e_i = [x, y, t, p]$, \n",
    "\n",
    "in which:\n",
    "* $(x,y)$ are the coordinates of the pixel that generated the event\n",
    "* $t$ is the relative timestamp at which the event occurred (in microseconds)\n",
    "* $p$ is the polarity of the event (*True* for an **ON** event, *False* for an **OFF** event)\n",
    "\n",
    "The polarity of an event indicates whether the contrast at a given pixel $(x,y)$ increased (indicated by an **ON** event) or decreased (indicated by an **OFF** event) at the time given by timestamp $t$. The timestamp for each event indicates the relative time at which the event was generated by the sensor. This is a relative timestamp with microsecond resolution. It can directly be used to measure the time between events, but is referenced to an arbitrary point in time (often, but not always, the time at which the camera was first powered on).\n",
    "\n",
    "**Note**: The timestamps in the provided recordings start at 0. This is not always the case. Always check the the timestamp of the first event in a recording to ensure that you have the correct time offset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with timestamps\n",
    "\n",
    "In usual operation, you can assume that the timestamps always increase. Therefore, we can use the timestamps to easily calculate the length of the recording:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import event_stream\n",
    "import numpy\n",
    "\n",
    "decoder = event_stream.Decoder(\"Data/WSU/two_horses.es\")\n",
    "\n",
    "# Read all the events into one large matrix\n",
    "events = numpy.concatenate([packet for packet in decoder])\n",
    "\n",
    "print(f\"Total events: {len(events)}\")\n",
    "print(f\"First timstamp: {events[0]['t']}\")\n",
    "print(f\"Last timestamp: {events[-1]['t']}\")\n",
    "\n",
    "duration = events[-1]['t'] - events[0]['t']\n",
    "print(f\"Recording length: {duration / 1e6} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timestamps from an event-based recording contain a lot of useful information on their own. As event-based cameras produce events in response to visual changes in the scene, the rate at which events are generated acts as an indicator for the activity in the scene. If we plot the timestamps as a vector (i.e. against their index), we get a plot in which the gradient of the line indicates the activity in the scene. \n",
    "\n",
    "We can use the code below to generate a simple plot of the data in the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import event_stream\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "decoder = event_stream.Decoder(\"Data/WSU/two_horses.es\")\n",
    "\n",
    "timestamps = []\n",
    "for packet in decoder:\n",
    "    for t, x, y, p in packet:\n",
    "        timestamps.append(t)\n",
    "\n",
    "plt.plot(timestamps)\n",
    "plt.xlabel(\"Event Index\")\n",
    "plt.ylabel(\"Time (us)\")\n",
    "plt.title(\"Plot of Timestamps vs. Index\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing algorithms for event-based processing\n",
    "\n",
    "You may have noticed that this code is not terribly efficient. For instance, we could have plotted the curve above by loading all the events into a single matrix as we did in the prior example.  Instead, the code iterates over the events and processes events individually. This method, whilst not efficient for this data, is important in understanding the event-based nature of the data and how to process it in an event-based way. This becomes especially critical when designing and building event-based systems to run in real-time and for real-world applications.\n",
    "\n",
    "The iterative approach above processes each event individually and in the order in which they are generated. Loading in all the data at once works for pre-recorded and offline files, but can quickly cause problems with large files or when the event-rate from the camera suddenly increases. This method also does not easily extend to real-time systems, when data needs to be processed as it is generated by the sensor. We can batch the events, but then we need to decide if we want to group them temporally (i.e. over a set period of time) or by volume (i.e. wait for a fixed number of events to occur). Both of these approaches can introduce latency and complexity into a processing system.\n",
    "\n",
    "Iterative approaches solve many of these problems, especially when the actions performed on each event are quick to execute. They will wait if there are no events to process and can buffer events if they occur faster than they can be processed. This provides the added benefit of activity-driven processing, where data processing is only performed when events are generated. This style of processing is well suited to high-speed hardware implementations, such such as a Field Gate Programmable Array (FPGA) or dedicated neuromorphic hardware accelerators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Global Activity: An iterative algorithm for better event-rate visualisation \n",
    "\n",
    "Whilst this plot of the timestamps is interesting, it only shows the speed at which the event rates are changing. This does not give us a sense of what is happening in the data.\n",
    "\n",
    "As it turns out, calculating a meaningful rate from non-uniformly sampled data is a hard problem. One possibility is to use time bins, but these bins cannot be shorter than the integration window. For an integration window of 200 ms, you can only generate 5 event rate samples per second, which is a very coarse approximation.\n",
    "\n",
    "Another option is a sliding window. The approximation is much better but it requires a lot of memory (usually a First-In-First-Out (FIFO) buffer). Extending the implementation to event-based data is not a straightforward task.\n",
    "\n",
    "To overcome this, we need to use something that calculates a value similar in meaning to the event rate, but one that can be efficiently implemented and calculated for event-based data. We call this the Global Activity Rate. To do this, we implement a simple leaky integrator in which we do the following:\n",
    "\n",
    "* Exponentially decay the previous activity over the inter-event time interval (***delta_t***). This is the **leak**.\n",
    "* Increment the activity by 1 for each event. This is the **integration**.\n",
    "\n",
    "We implement the leak as follows:\n",
    "\n",
    "$ activity = e^{((-\\Delta t) / \\tau)}$\n",
    "\n",
    "Where $\\Delta t$ is the increase in time since the last event, and $\\tau$ is a fixed time interval over which we want to decay the value. We can pick $\\tau$ to change the time intervals of interest. It is worth exploring the effect of changing $tau$ on the graph produced.\n",
    "\n",
    "The beauty of this algorithm is that it uses only two memory variables (both ***floats***). This makes the algorithm much faster and memory efficient than other event-rate algorithm.\n",
    "\n",
    "**Note:** Some of you might notice that these equations look very similar to those for a leaky-integrate-and-fire neuron model. This is because it shares the same underlying structure! If you add a threshold to trigger an action when the global activity increases beyond a certain value (with a refractory period for good measure), you end up with a full-fledged leaky integrate-and-fire neuron.\n",
    "\n",
    "We can calculate the Global Activity using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import event_stream\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "decoder = event_stream.Decoder(\"Data/WSU/two_horses.es\")\n",
    "\n",
    "event_activity = 0.0\n",
    "previous_t = 0      # microseconds (uS)\n",
    "tau = 100.0         # microseconds (uS)\n",
    "\n",
    "ts = []\n",
    "activity = []\n",
    "for packet in decoder:\n",
    "    for t, x, y, p in packet:\n",
    "        delta_t = t - previous_t\n",
    "        event_activity *= math.exp(-float(delta_t) / tau)   # Leak\n",
    "        event_activity += 1                                 # Integrate\n",
    "        previous_t = t\n",
    "        ts.append(t)\n",
    "        activity.append(event_activity)\n",
    "\n",
    "plt.plot(ts, activity)\n",
    "plt.xlabel(\"Timestamps (microseconds)\")\n",
    "plt.ylabel(\"Activity\")\n",
    "plt.title(\"Plot of Global Activity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on visualising event-based data\n",
    "\n",
    "One of the challenges with event-based data is that the raw recordings are not in a format that is readily accessible for viewing. Unlike the output of a conventional camera, there are no straightforward ways to replay a recording. There are no mature tools for viewing, or even working, with event-based camera data. Instead, the data is often imported into a programming environment and then converted to a visual representation using custom-written scripts. The field also has not yet created any standard methods for storing, processing, or presenting data.\n",
    "\n",
    "Frames present the most immediate and compelling method to visualise event-based data given that their format matches the output format of most monitors and displays (which are themselves frame-based). However, creating a 2D representation of event-based data requires a post-processing step that inherently discards or aggregates data and the choice of post-processing algorithm can have a significant impact on the interpretation of the data. \n",
    "\n",
    "**Note:** The process of generating frames from event-based data can hide important aspects of the data itself, or lend itself to incorrect interpretation of the true motion in the scene. Generating frames produces a single view on the data. Other views may exist and may be more suitable to the underlying data. This may also vary from application to application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating frames from event-based data\n",
    "In this tutorial, we will explore a simple method for converting event-based data into frames. This process will render conventional frames from the event-stream and compile those into an animation or a video, allowing us to interact with the data in a familiar format. These techniques are often used to explore new event-based data, to perform quick checks on the validity of data, and to diagnose and debug algorithms. Keep in mind that this is not a complete representative of the underlying data stream.\n",
    "\n",
    "Most methods for converting event-based data into frames makes use of the following set of steps:\n",
    "\n",
    "1. Identify a set time interval over which to generate frames\n",
    "2. Place events into time bins determined by the time intervals\n",
    "3. Accumulate the events in each time bin into a 2D frame representation \n",
    "4. Collate the frame from each time interval into a video\n",
    "\n",
    "A multitude of ways exist to perform the third step in the above process, and the choice of algorithm is highly dependent on the nature of the data to be visualise. The conversion of the list of spatio-temporal events to a 2D frame is inherently lossy. \n",
    "\n",
    "For our simple method, we will keep track of all the pixels that have events occurring during each time bin. We will then mark them as either containing an ON event or an OFF event based on the last event received for that pixel in each time interval. This will result in a frame for each time interval showing the pixels at which events occurred.\n",
    "\n",
    "The following code renders simple frames from the event-stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import event_stream\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "\n",
    "decoder = event_stream.Decoder(\"Data/WSU/two_horses.es\")\n",
    "events = np.concatenate([packet for packet in decoder])\n",
    "\n",
    "frame_duration = 1000000    # µs\n",
    "next_frame_t = 0            # µs\n",
    "\n",
    "frame = np.zeros((decoder.width, decoder.height))\n",
    "frames = []\n",
    "\n",
    "# Generate the frames from the event stream\n",
    "for t, x, y, p in events:\n",
    "    if t >= next_frame_t:\n",
    "        frames.append(frame)\n",
    "        frame = np.zeros((decoder.width, decoder.height)) \n",
    "        next_frame_t += frame_duration\n",
    "    frame[x,y] = 1 if p == True else -1\n",
    "\n",
    "# Display the frames inline using Matplotlib Animations\n",
    "fig, ax = plt.subplots()\n",
    "canvas = ax.imshow(frame.T, origin='lower')\n",
    "def animate(i):\n",
    "    canvas.set_data(frames[i].T)\n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=len(frames))\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises to try:\n",
    "\n",
    "* Try changing the ***frame_duration*** to see the effect of increasing the frame rate on the data. \n",
    "* Try implementing an alternative frame conversion method that counts the number of events at each pixel. \n",
    "* Try blending the current frame with a weighted version of the previous frame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise and filtering\n",
    "\n",
    "You may have noticed that the recordings on the camera contain noise. From the event-rate curves, it is clear that the cameras generate events even when there is no explicit activity in the scene. This noise is also visible in the frames as individual events that seemingly occur randomly. Whilst there are different types of noise events,  we will examine two of them in these tutorials:\n",
    "\n",
    "* **Spurious events** are random events from pixels that do not necessarily correspond to a contrast change.\n",
    "* **Hot pixels** are noisy pixels that generate lots of events\n",
    "\n",
    "These are caused by a variety of factors, including photodiode noise, transistor mismatch in the circuitry, internal noise in the sensor, temperature effects. The nature and cause of these noise events are an active subject of research in the neuromorphic and event-based sensor community. \n",
    "\n",
    "There are two main approaches for dealing with noise:\n",
    "\n",
    "* Design algorithms that are tolerant of noise\n",
    "* Filter out the noise prior to processing\n",
    "\n",
    "In this tutorial, we will explore algorithms for pre-processing algorithms for filtering and removing these noise events.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neighbour support noise filtering\n",
    "\n",
    "A popular way to remove spurious background noise events is to use a temporal neighbour support filtering algorithm. This method works by eliminating isolated events that do not occur near other event, both spatially and temporally. The algorithm is iterative and is often implemented in hardware directly. \n",
    "\n",
    "The algorithm works by keeping track of the last time each pixel in the camera generated an event. When a new event occurs, the neighbouring pixels are checked to see if an event occurred within an specified time window. If none of the neighbouring events have generated an event recently, the event is labelled as a noise event and discarded. If at least one of the neighbouring pixels has generated an event recently, then the pixel is said to be supported and is retained. \n",
    "\n",
    "This filtering process is illustrated below:\n",
    "\n",
    "![SegmentLocal](images/noise_filtering_illustration.png \"segment\")\n",
    "\n",
    "This filter is very easy to implement and works well with many scenes but it does have some drawbacks. \n",
    "\n",
    "Firstly, it is worth noting that this filter may incorrectly discard the first event from a valid motion across the sensor surface. In most scenes, this is not noticeable as there are usually enough subsequent events to compensate for it. However, when tracking objects such as faint stars across the sensor surface, these lost events may be significant and this filter may not be the appropriate choice. \n",
    "\n",
    "The time window selected for the filter is particularly important and can greatly affect the efficacy of the filtering process. A longer time window will generally keep more events.\n",
    "\n",
    "The neighbour support noise filter can be implemented as follows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import event_stream\n",
    "import numpy\n",
    "\n",
    "time_window = 10000         # µs, a shorter time window filters more events\n",
    "frame_duration = 1000000    # µs (used for plotting the events as a video)\n",
    "\n",
    "# Read in all the events from the file\n",
    "decoder = event_stream.Decoder(\"data/two_horses.es\")\n",
    "events = numpy.concatenate([packet for packet in decoder])\n",
    "\n",
    "# Filter the events by examining the neighbouring four pixels to see if an event\n",
    "# has occurred within the specified time_window. If so, keep that event.\n",
    "ts = np.zeros((decoder.width, decoder.height))\n",
    "filtered_events = []\n",
    "for t, x, y, on in events:\n",
    "    ts[x][y] = t\n",
    "    if (\n",
    "        (x > 0 and ts[x - 1][y] + time_window > t)\n",
    "        or (x < decoder.width - 1 and ts[x + 1][y] + time_window > t)\n",
    "        or (y > 0 and ts[x][y - 1] + time_window > t)\n",
    "        or (y < decoder.height - 1 and ts[x][y + 1] + time_window > t)\n",
    "    ):\n",
    "        filtered_events.append((t, x, y, on))\n",
    "\n",
    "# Print a summary of the filtering results \n",
    "print(f\"Original recording contained {len(events)} events.\")\n",
    "print(f\"Filtered recording contains {len(filtered_events)}.\")\n",
    "print(f\"Reduced by { ((len(events) - len(filtered_events)) / len(events)) * 100 }%\")\n",
    "\n",
    "\n",
    "# Generate the frames from the event stream\n",
    "next_frame_t = 0            # µs\n",
    "frame = numpy.zeros((decoder.width, decoder.height))\n",
    "frames = []\n",
    "for t, x, y, p in filtered_events:\n",
    "    if t >= next_frame_t:\n",
    "        frames.append(frame)\n",
    "        frame = numpy.zeros((decoder.width, decoder.height)) \n",
    "        next_frame_t += frame_duration\n",
    "    frame[x,y] = 1 if p == True else -1\n",
    "\n",
    "# Display the frames inline using Matplotlib Animations\n",
    "fig, ax = plt.subplots()\n",
    "plt.title(\"Filtered Event Frames\")\n",
    "canvas = ax.imshow(frame.T, origin='lower')\n",
    "def animate(i):\n",
    "    canvas.set_data(frames[i].T)\n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=len(frames))\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises to try:\n",
    "* Implement the full search of the 3x3 and 4x4 area around the event. Numpy array slicing can perform this operation very efficiently.\n",
    "* Explore requiring more than one pixel of support when filtering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An event-based filtering pipeline\n",
    "\n",
    "Finally, the above filter can be used to build an entirely event-based building block that ingests events and emits events. In the code below, the `event_stream` library is used to both read the events and to write the filtered events out to a new file. Once this is done, other algorithms can then operate on the filtered events without any knowledge of the upstream processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import event_stream\n",
    "import numpy\n",
    "\n",
    "time_window = 1000  # µs, a shorter time window filters more events\n",
    "\n",
    "decoder = event_stream.Decoder(\"Data/WSU/two_horses.es\")\n",
    "encoder = event_stream.Encoder(\n",
    "    \"Data/WSU/two_horses_denoised.es\", decoder.type, decoder.width, decoder.height\n",
    ")\n",
    "\n",
    "ts = [[-time_window] * decoder.height for _ in range(0, decoder.width)]\n",
    "\n",
    "for packet in decoder:\n",
    "    new_packet = []\n",
    "    for t, x, y, on in packet:\n",
    "        ts[x][y] = t\n",
    "        if (\n",
    "            (x > 0 and ts[x - 1][y] + time_window > t)\n",
    "            or (x < decoder.width - 1 and ts[x + 1][y] + time_window > t)\n",
    "            or (y > 0 and ts[x][y - 1] + time_window > t)\n",
    "            or (y < decoder.height - 1 and ts[x][y + 1] + time_window > t)\n",
    "        ):\n",
    "            new_packet.append((t, x, y, on))\n",
    "    encoder.write(numpy.array(new_packet, dtype=event_stream.dvs_dtype))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "07c4f84319129d1c976bad471c8642b46ea9b7b96fed5777f2c683a8a5a74ac2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
